{"links": {"self": {"href": "data/repositories/dedalus-project/dedalus/pullrequests/35/comments/12481777.json"}, "html": {"href": "#!/dedalus-project/dedalus/pull-requests/35/_/diff#comment-12481777"}}, "deleted": false, "pullrequest": {"type": "pullrequest", "id": 35, "links": {"self": {"href": "data/repositories/dedalus-project/dedalus/pullrequests/35.json"}, "html": {"href": "#!/dedalus-project/dedalus/pull-requests/35"}}, "title": "Simple Checkpointing mechanism"}, "content": {"raw": "Jeff,\n    I've been testing out checkpointing in this pull request.  The checkpointing itself (and restarts) works great (thanks Jeff!!!), but I have come across a very odd problem for other outputs.  When I'm doing runs using this PR (I have disabled checkpointing in the run in question, to try and narrow this down), I think that only the first set of normal analysis outputs respects the max_writes/file. \n\nIn particular, I'm doing 20 writes per file normally.  On the main branch of dedalus, this works as intended (just tested) and each file contains exactly 20 writes.  When I switch over to this PR and run the same script, the first output file (set 1) contains 20 writes as intended, but then the second output file contains arbitrarily many writes (all further writes go into set 2).\n\nHere's an h5dump on one of my output files (p0 of 256) in set 1 showing the expected 20 writes/file:\n\n\n```\n#!bash\n\n    h5dump FC_fixed_nrhocz2_Ra1e8_test/slices/slices_s1/slices_s1_p0.h5\n    <...>\n       ATTRIBUTE \"writes\" {\n          DATATYPE  H5T_STD_I64LE\n          DATASPACE  SCALAR\n          DATA {\n          (0): 20\n          }\n       }\n    <...>\n\n```\n\n\nand here is the same in set 2, showing an unexepected 69 writes (corresponding to when I killed the run; it would just keep accumulating otherwise in this same set):\n\n\n```\n#!bash\n\n     h5dump FC_fixed_nrhocz2_Ra1e8_test/slices/slices_s2/slices_s2_p0.h5\n       <...>\n       ATTRIBUTE \"writes\" {\n          DATATYPE  H5T_STD_I64LE\n          DATASPACE  SCALAR\n          DATA {\n          (0): 69\n          }\n       }\n\n    <...> \n\n```\n\nBizarrely, if I do the same tests and include checkpointing, checkpoint files respect the max_writes=1/file, and I get regular checkpoint sets output even though the other output files don't (e.g., I have 48 checkpoint sets, s1 -- s48, but still only 2 normal analysis sets, s1 and s2, with 20 writes in s1 and arbitrarily many in s2).\n\nAny ideas where this is coming from?  I'm starting to dive into the PR diffs and my best guess is in evaluator.py.\n\nSorry this isn't more clearly defined; it came up while doing production runs.\n\nWill let you know if I figure anything out,\n--Ben", "markup": "markdown", "html": "<p>Jeff,\n    I've been testing out checkpointing in this pull request.  The checkpointing itself (and restarts) works great (thanks Jeff!!!), but I have come across a very odd problem for other outputs.  When I'm doing runs using this PR (I have disabled checkpointing in the run in question, to try and narrow this down), I think that only the first set of normal analysis outputs respects the max_writes/file. </p>\n<p>In particular, I'm doing 20 writes per file normally.  On the main branch of dedalus, this works as intended (just tested) and each file contains exactly 20 writes.  When I switch over to this PR and run the same script, the first output file (set 1) contains 20 writes as intended, but then the second output file contains arbitrarily many writes (all further writes go into set 2).</p>\n<p>Here's an h5dump on one of my output files (p0 of 256) in set 1 showing the expected 20 writes/file:</p>\n<div class=\"codehilite language-bash\"><pre><span></span>    h5dump FC_fixed_nrhocz2_Ra1e8_test/slices/slices_s1/slices_s1_p0.h5\n    &lt;...&gt;\n       ATTRIBUTE <span class=\"s2\">&quot;writes&quot;</span> <span class=\"o\">{</span>\n          DATATYPE  H5T_STD_I64LE\n          DATASPACE  SCALAR\n          DATA <span class=\"o\">{</span>\n          <span class=\"o\">(</span><span class=\"m\">0</span><span class=\"o\">)</span>: <span class=\"m\">20</span>\n          <span class=\"o\">}</span>\n       <span class=\"o\">}</span>\n    &lt;...&gt;\n</pre></div>\n\n\n<p>and here is the same in set 2, showing an unexepected 69 writes (corresponding to when I killed the run; it would just keep accumulating otherwise in this same set):</p>\n<div class=\"codehilite language-bash\"><pre><span></span>     h5dump FC_fixed_nrhocz2_Ra1e8_test/slices/slices_s2/slices_s2_p0.h5\n       &lt;...&gt;\n       ATTRIBUTE <span class=\"s2\">&quot;writes&quot;</span> <span class=\"o\">{</span>\n          DATATYPE  H5T_STD_I64LE\n          DATASPACE  SCALAR\n          DATA <span class=\"o\">{</span>\n          <span class=\"o\">(</span><span class=\"m\">0</span><span class=\"o\">)</span>: <span class=\"m\">69</span>\n          <span class=\"o\">}</span>\n       <span class=\"o\">}</span>\n\n    &lt;...&gt; \n</pre></div>\n\n\n<p>Bizarrely, if I do the same tests and include checkpointing, checkpoint files respect the max_writes=1/file, and I get regular checkpoint sets output even though the other output files don't (e.g., I have 48 checkpoint sets, s1 -- s48, but still only 2 normal analysis sets, s1 and s2, with 20 writes in s1 and arbitrarily many in s2).</p>\n<p>Any ideas where this is coming from?  I'm starting to dive into the PR diffs and my best guess is in evaluator.py.</p>\n<p>Sorry this isn't more clearly defined; it came up while doing production runs.</p>\n<p>Will let you know if I figure anything out,\n--Ben</p>", "type": "rendered"}, "created_on": "2015-12-05T21:40:25.867394+00:00", "user": {"display_name": "Benjamin Brown", "uuid": "{7ccecdb3-3639-4001-8249-060e80320bda}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B7ccecdb3-3639-4001-8249-060e80320bda%7D"}, "html": {"href": "https://bitbucket.org/%7B7ccecdb3-3639-4001-8249-060e80320bda%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:0696c4b9-e94c-41ac-82be-62ad4f0ec571/8bc6f4da-871a-48b1-88ea-998663d18142/128"}}, "nickname": "Benjamin Brown", "type": "user", "account_id": "557058:0696c4b9-e94c-41ac-82be-62ad4f0ec571"}, "updated_on": "2015-12-05T21:40:25.869223+00:00", "type": "pullrequest_comment", "id": 12481777}