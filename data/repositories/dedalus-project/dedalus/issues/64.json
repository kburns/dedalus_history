{"priority": "major", "kind": "bug", "repository": {"links": {"self": {"href": "data/repositories/dedalus-project/dedalus.json"}, "html": {"href": "#!/dedalus-project/dedalus"}, "avatar": {"href": "data/bytebucket.org/ravatar/{1dc39ab6-2798-450d-af2f-e976f94b5794}ts=2009435"}}, "type": "repository", "name": "dedalus", "full_name": "dedalus-project/dedalus", "uuid": "{1dc39ab6-2798-450d-af2f-e976f94b5794}"}, "links": {"attachments": {"href": "data/repositories/dedalus-project/dedalus/issues/64/attachments_page=1.json"}, "self": {"href": "data/repositories/dedalus-project/dedalus/issues/64.json"}, "watch": {"href": "https://api.bitbucket.org/2.0/repositories/dedalus-project/dedalus/issues/64/watch"}, "comments": {"href": "data/repositories/dedalus-project/dedalus/issues/64/comments_page=1.json"}, "html": {"href": "#!/dedalus-project/dedalus/issues/64/nccs-cause-strange-mpi-crashes-after"}, "vote": {"href": "https://api.bitbucket.org/2.0/repositories/dedalus-project/dedalus/issues/64/vote"}}, "reporter": {"display_name": "Evan Anders", "uuid": "{ce45a673-fcc9-48fc-a547-f4ee9f3a8020}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Bce45a673-fcc9-48fc-a547-f4ee9f3a8020%7D"}, "html": {"href": "https://bitbucket.org/%7Bce45a673-fcc9-48fc-a547-f4ee9f3a8020%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:320d4568-38ec-40ab-9264-ac9b3760572a/7e7ec364-57a7-4b6a-809c-b17a66066163/128"}}, "nickname": "evanhanders", "type": "user", "account_id": "557058:320d4568-38ec-40ab-9264-ac9b3760572a"}, "title": "NCCs cause strange MPI crashes after commit 4232435 in 3D domains with distributed processor meshes", "component": null, "votes": 0, "watches": 1, "content": {"raw": "After [this commit](#!/dedalus-project/dedalus/commits/4232435181b5989df78ff6d6ce07e6cc4093cf54), my 3D equation sets with significant NCCs are failing when setting up file handlers when I parallelize over two directions. The failures usually show up in a few sorts of barfy Tracebacks or as seg faults a bit further down in my script. Here\u2019s one such traceback.\r\n\r\n> Traceback \\(most recent call last\\):  \r\n> File \"rayleigh\\_benard.py\", line 113, in <module>  \r\n> snap = solver.evaluator.add\\_file\\_handler\\('snapshots', sim\\_dt=0.2, max\\_writes=10\\)  \r\n> File \"/home/evan/research/dedalus/src/dedalus/dedalus/core/evaluator.py\", line 69, in add\\_file\\_handler  \r\n> FH = FileHandler\\(filename, self.domain, self.vars, \\*\\*kw\\)  \r\n> File \"/home/evan/research/dedalus/src/dedalus/dedalus/core/evaluator.py\", line 397, in **init**  \r\n> self.set\\_num = comm.bcast\\(set\\_num, root=0\\)  \r\n> File \"mpi4py/MPI/Comm.pyx\", line 1257, in mpi4py.MPI.Comm.bcast  \r\n> File \"mpi4py/MPI/msgpickle.pxi\", line 631, in mpi4py.MPI.PyMPI\\_bcast  \r\n> File \"mpi4py/MPI/msgpickle.pxi\", line 632, in mpi4py.MPI.PyMPI\\_bcast  \r\n> mpi4py.MPI.Exception: MPI\\_ERR\\_TRUNCATE: message truncated\r\n\r\nWhat seems to be happening is that the bcast calls on lines 397-398 of core/evaluator.py aren\u2019t working properly, and when I have a processor mesh of shape \\[N, M\\], I get this error on N processors. Usually the last processor in the first direction \\(e.g., on 64 cores, with mesh \\[4, 16\\], this error happens on process 15, 31, 46, 63\\).\r\n\r\nI\u2019m not sure what\u2019s causing this on the dedalus side, but I think it has something to do with the update to the NCC multiplication routine in commit 4232435. Sometimes, the specific MPI traceback that I get is also apparently a known bug in mpi4py \\([see here](https://bitbucket.org/mpi4py/mpi4py/issues/17/negative-size-passed-to)\\), and can supposedly be fixed by changing from lowercase bcast to uppercase Bcast. Looks like an integer overflow during the pickling routine in bcast.\r\n\r\nThis error DOES NOT happen in 2D with equation sets with similar NCCs, nor does it happen when I don\u2019t specify a mesh. I\u2019m attaching two scripts of moderately modified versions of the 3D rayleigh benard example which reproduce the bug for me.  I\u2019ve added a NCC scaling to most of the equations \\(see lines 50-52, 67-70\\). I\u2019ve also switched from SinCos bases to Fourier, which I use in my personal problems. Otherwise, the files should be identical to the original example. Running mesh\\_rayleigh\\_benard.py on 4 processes breaks for me, but running noMesh\\_rayleigh\\_benard.py does not.\r\n\r\nThis bug basically makes it so that all of our fully compressible equation implementations break in well-parallelized, 3D problems on the tip version of dedalus.", "markup": "markdown", "html": "<p>After <a data-is-external-link=\"true\" href=\"#!/dedalus-project/dedalus/commits/4232435181b5989df78ff6d6ce07e6cc4093cf54\" rel=\"nofollow\">this commit</a>, my 3D equation sets with significant NCCs are failing when setting up file handlers when I parallelize over two directions. The failures usually show up in a few sorts of barfy Tracebacks or as seg faults a bit further down in my script. Here\u2019s one such traceback.</p>\n<blockquote>\n<p>Traceback (most recent call last):<br />\nFile \"rayleigh_benard.py\", line 113, in &lt;module&gt;<br />\nsnap = solver.evaluator.add_file_handler('snapshots', sim_dt=0.2, max_writes=10)<br />\nFile \"/home/evan/research/dedalus/src/dedalus/dedalus/core/evaluator.py\", line 69, in add_file_handler<br />\nFH = FileHandler(filename, self.domain, self.vars, **kw)<br />\nFile \"/home/evan/research/dedalus/src/dedalus/dedalus/core/evaluator.py\", line 397, in <strong>init</strong><br />\nself.set_num = comm.bcast(set_num, root=0)<br />\nFile \"mpi4py/MPI/Comm.pyx\", line 1257, in mpi4py.MPI.Comm.bcast<br />\nFile \"mpi4py/MPI/msgpickle.pxi\", line 631, in mpi4py.MPI.PyMPI_bcast<br />\nFile \"mpi4py/MPI/msgpickle.pxi\", line 632, in mpi4py.MPI.PyMPI_bcast<br />\nmpi4py.MPI.Exception: MPI_ERR_TRUNCATE: message truncated</p>\n</blockquote>\n<p>What seems to be happening is that the bcast calls on lines 397-398 of core/evaluator.py aren\u2019t working properly, and when I have a processor mesh of shape [N, M], I get this error on N processors. Usually the last processor in the first direction (e.g., on 64 cores, with mesh [4, 16], this error happens on process 15, 31, 46, 63).</p>\n<p>I\u2019m not sure what\u2019s causing this on the dedalus side, but I think it has something to do with the update to the NCC multiplication routine in commit <a href=\"#!/dedalus-project/dedalus/commits/4232435\" rel=\"nofollow\" class=\"ap-connect-link\">4232435</a>. Sometimes, the specific MPI traceback that I get is also apparently a known bug in mpi4py (<a data-is-external-link=\"true\" href=\"https://bitbucket.org/mpi4py/mpi4py/issues/17/negative-size-passed-to\" rel=\"nofollow\">see here</a>), and can supposedly be fixed by changing from lowercase bcast to uppercase Bcast. Looks like an integer overflow during the pickling routine in bcast.</p>\n<p>This error DOES NOT happen in 2D with equation sets with similar NCCs, nor does it happen when I don\u2019t specify a mesh. I\u2019m attaching two scripts of moderately modified versions of the 3D rayleigh benard example which reproduce the bug for me.  I\u2019ve added a NCC scaling to most of the equations (see lines 50-52, 67-70). I\u2019ve also switched from SinCos bases to Fourier, which I use in my personal problems. Otherwise, the files should be identical to the original example. Running mesh_rayleigh_benard.py on 4 processes breaks for me, but running noMesh_rayleigh_benard.py does not.</p>\n<p>This bug basically makes it so that all of our fully compressible equation implementations break in well-parallelized, 3D problems on the tip version of dedalus.</p>", "type": "rendered"}, "assignee": null, "state": "resolved", "version": null, "edited_on": null, "created_on": "2019-06-27T09:36:05.063347+00:00", "milestone": null, "updated_on": "2019-09-13T15:16:09.865128+00:00", "type": "issue", "id": 64}