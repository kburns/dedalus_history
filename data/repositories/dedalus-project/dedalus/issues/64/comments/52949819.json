{"links": {"self": {"href": "data/repositories/dedalus-project/dedalus/issues/64/comments/52949819.json"}, "html": {"href": "#!/dedalus-project/dedalus/issues/64#comment-52949819"}}, "issue": {"links": {"self": {"href": "data/repositories/dedalus-project/dedalus/issues/64.json"}}, "type": "issue", "id": 64, "repository": {"links": {"self": {"href": "data/repositories/dedalus-project/dedalus.json"}, "html": {"href": "#!/dedalus-project/dedalus"}, "avatar": {"href": "data/bytebucket.org/ravatar/{1dc39ab6-2798-450d-af2f-e976f94b5794}ts=2009435"}}, "type": "repository", "name": "dedalus", "full_name": "dedalus-project/dedalus", "uuid": "{1dc39ab6-2798-450d-af2f-e976f94b5794}"}, "title": "NCCs cause strange MPI crashes after commit 4232435 in 3D domains with distributed processor meshes"}, "content": {"raw": "Is it giving different error messages, or always the same as the one above?  My current flatiron stack uses openmpi-2.1.6, hdf5-1.8.21, fftw-3.3.8, mpi4py-3.0.2, h5py-2.9.0.", "markup": "markdown", "html": "<p>Is it giving different error messages, or always the same as the one above?  My current flatiron stack uses openmpi-2.1.6, hdf5-1.8.21, fftw-3.3.8, mpi4py-3.0.2, h5py-2.9.0.</p>", "type": "rendered"}, "created_on": "2019-07-11T20:57:01.102444+00:00", "user": {"display_name": "Keaton Burns", "uuid": "{3d3e64f1-bf12-45df-b655-4543d8fb34c4}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B3d3e64f1-bf12-45df-b655-4543d8fb34c4%7D"}, "html": {"href": "https://bitbucket.org/%7B3d3e64f1-bf12-45df-b655-4543d8fb34c4%7D/"}, "avatar": {"href": "https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/557058:e31a7835-5317-4dfa-8551-f32a06f40279/40f33f99-2b75-4a17-a8c2-07c8d74c7480/128"}}, "nickname": "kburns", "type": "user", "account_id": "557058:e31a7835-5317-4dfa-8551-f32a06f40279"}, "updated_on": null, "type": "issue_comment", "id": 52949819}